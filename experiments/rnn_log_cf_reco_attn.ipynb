{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20Mn data MovieLens Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An experiment on 20mn MovieLens Dataset.\n",
    "\n",
    "The core idea is to use rnn to process sequence log data, to replace trained user embedding on crossfiltering nn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[task:experiment with RNN+CF on movielens 20m data>>start]<2018-10-22_10:10:33|0s,0s>\t\n"
     ]
    }
   ],
   "source": [
    "from ray.lprint import lprint\n",
    "l = lprint(\"experiment with RNN+CF on movielens 20m data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[has GPU cuda]<2018-10-22_10:10:37|3s,3s>\tTrue\n"
     ]
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "SEQ_LEN = 19 # sequence length\n",
    "DIM = 100 # hidden vector lenth, embedding length\n",
    "l.p(\"has GPU cuda\",CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %ls /data/ml-20m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"/data/ml-20m/ratings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loading csv file]<2018-10-22_10:10:41|4s,8s>\t/data/ml-20m/ratings.csv\n",
      "[csv file loaded]<2018-10-22_10:10:49|7s,16s>\t\n"
     ]
    }
   ],
   "source": [
    "l.p(\"loading csv file\", DATA)\n",
    "rate_df = pd.read_csv(DATA)\n",
    "l.p(\"csv file loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000263"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate_df.groupby(\"userId\").count()[\"movieId\"].min()\n",
    "# The minimum number of movies a user have rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of users and movies:\t 138493 \t 26744\n"
     ]
    }
   ],
   "source": [
    "userId = list(set(rate_df[\"userId\"]))\n",
    "movieId = list(set(rate_df[\"movieId\"]))\n",
    "print(\"total number of users and movies:\\t\",len(userId),\"\\t\",len(movieId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[making dictionary]<2018-10-22_10:10:53|3s,19s>\t\n"
     ]
    }
   ],
   "source": [
    "l.p(\"making dictionary\")\n",
    "u2i = dict((v,k) for k,v in enumerate(userId))\n",
    "m2i = dict((v,k) for k,v in enumerate(movieId))\n",
    "i2u = dict((k,v) for k,v in enumerate(userId))\n",
    "i2m = dict((k,v) for k,v in enumerate(movieId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating original index to the new index\n",
    "rate_df[\"movieIdx\"] = rate_df.movieId.apply(lambda x:m2i[x]).astype(int)\n",
    "rate_df[\"userIdx\"] = rate_df.userId.apply(lambda x:u2i[x]).astype(int)\n",
    "rate_df[\"rating\"] = rate_df[\"rating\"]/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train /Valid Split: K fold Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generating groubby slice]<2018-10-22_10:10:04|791s,811s>\t\n"
     ]
    }
   ],
   "source": [
    "l.p(\"generating groubby slice\")\n",
    "def get_user_trail(rate_df):\n",
    "    return rate_df.sort_values(by=[\"userId\",\"timestamp\"]).groupby(\"userId\")\n",
    "    #gb.apply(lambda x:x.sample(n = 20, replace = False))\n",
    "gb = get_user_trail(rate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_CONSEQ = True\n",
    "\n",
    "if KEEP_CONSEQ:\n",
    "    # keep the consequtivity among the items the user has rated\n",
    "    def sample_split(x):\n",
    "        sample_idx = math.floor(np.random.rand()*(len(x) - SEQ_LEN - 1))\n",
    "        seq_and_y = x[sample_idx:sample_idx + SEQ_LEN+1]\n",
    "        return seq_and_y\n",
    "else:\n",
    "    # randomly pick the right amount of sample from user's record\n",
    "    pick_k = np.array([0]*SEQ_LEN +[1])==1\n",
    "\n",
    "    def sample_split(x):\n",
    "        sampled = x.sample(n = 20, replace = False)\n",
    "        seq = sampled.head(19).sort_values(by=\"timestamp\")\n",
    "        y = sampled[pick_k]\n",
    "        return pd.concat([seq,y])\n",
    "\n",
    "class rnn_record(Dataset):\n",
    "    def __init__(self, gb):\n",
    "        \"\"\"\n",
    "        A pytorch dataset object designed to group logs into user behavior sequence\n",
    "        \"\"\"\n",
    "        self.gb = gb\n",
    "        self.make_seq()\n",
    "    \n",
    "    def make_seq(self):\n",
    "        \"\"\"\n",
    "        Resample the data\n",
    "        \"\"\"\n",
    "        self.all_seq = self.gb.apply(sample_split)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.gb)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        \"\"\"\n",
    "        next(generator) will spit out the 'return' of this function\n",
    "        this is a single row in the batch\n",
    "        \"\"\"\n",
    "        df = self.all_seq.loc[idx]\n",
    "        seq = df.head(SEQ_LEN)[[\"movieIdx\",\"rating\"]].values\n",
    "        targ = df.head(SEQ_LEN+1).tail(1)[[\"movieIdx\",\"rating\"]].values\n",
    "        targ_v, targ_y =targ[:,0], targ[:,1]\n",
    "        return idx,seq,targ_v,targ_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data generator\n",
    "\n",
    "# data_gb = get_user_trail(rate_df)\n",
    "# rr = rnn_record(data_gb)\n",
    "# rr.all_seq\n",
    "\n",
    "# dl = DataLoader(rr,shuffle=True,batch_size=1)\n",
    "# gen = iter(dl)\n",
    "# next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model\n",
    "\n",
    "class mLinkNet(nn.Module):\n",
    "    def __init__(self, hidden_size,v_size):\n",
    "        \"\"\"\n",
    "        mLinkNet, short for missing link net\n",
    "        \"\"\"\n",
    "        super(mLinkNet,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.v_size = v_size\n",
    "        self.emb = nn.Embedding(v_size,hidden_size)\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size = self.hidden_size+1,\n",
    "                          hidden_size= hidden_size+1,\n",
    "                          num_layers=1,\n",
    "                          batch_first = True,\n",
    "                          dropout=0)\n",
    "        \n",
    "        self.mlp = nn.Sequential(*[\n",
    "            nn.Dropout(.3),\n",
    "            nn.Linear(hidden_size*2 + 1, 256, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256,1,bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        ])\n",
    "    \n",
    "    def forward(self,seq,targ_v):\n",
    "        seq_vec = torch.cat([self.emb(seq[:,0].long()),\n",
    "                             seq[:,1].unsqueeze(-1).float()], dim=2)\n",
    "        output, hn = self.rnn(seq_vec)\n",
    "        x = torch.cat([hn.squeeze(0),self.emb(targ_v.long()).squeeze(1)],dim=1)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def action(*args,**kwargs):\n",
    "    # get data from data feeder\n",
    "    idx,seq,targ_v,y = args[0]\n",
    "    if CUDA:\n",
    "        seq,targ_v,y = seq.cuda(),targ_v.cuda(),y.cuda()\n",
    "    y = y.float()\n",
    "    \n",
    "    # Clear the Jacobian Matrix\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # Predict y hat\n",
    "    y_ = mln(seq, targ_v)\n",
    "    # Calculate Loss\n",
    "    loss = loss_func(y_,y)\n",
    "    \n",
    "    # Backward Propagation\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # Mean Absolute Loss as print out metrics\n",
    "    mae = torch.mean(torch.abs(y_-y))\n",
    "    if kwargs[\"ite\"] == train_len - 1: # resample the sequence\n",
    "        trainer.train_data.dataset.make_seq()\n",
    "    return {\"loss\":loss.item(),\"mae\":mae.item()}\n",
    "\n",
    "def val_action(*args,**kwargs):\n",
    "    \"\"\"\n",
    "    A validation step\n",
    "    Exactly the same like train step, but no learning, only forward pass\n",
    "    \"\"\"\n",
    "    idx,seq,targ_v,y = args[0]\n",
    "    if CUDA:\n",
    "        seq,targ_v,y = seq.cuda(),targ_v.cuda(),y.cuda()\n",
    "    y = y.float()\n",
    "    \n",
    "    y_ = mln(seq, targ_v)\n",
    "    \n",
    "    loss = loss_func(y_,y)\n",
    "    mae = torch.mean(torch.abs(y_-y))\n",
    "    if kwargs[\"ite\"] == valid_len - 1:\n",
    "        trainer.val_data.dataset.make_seq()\n",
    "    return {\"loss\":loss.item(),\"mae\":mae.item()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[making train/test split]<2018-10-22_10:10:29|24s,835s>\t\n",
      "[start training]<2018-10-22_10:10:29|0s,835s>\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generating dataset]<2018-10-22_10:10:04|35s,870s>\ttrain\n",
      "[generating dataset]<2018-10-22_10:10:38|33s,904s>\tvalid\n",
      "[dataset generated]<2018-10-22_10:10:11|32s,937s>\t\n",
      "[creating model]<2018-10-22_10:10:11|0s,937s>\t\n",
      "[loading model to GPU]<2018-10-22_10:10:11|0s,937s>\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/4327 [00:00<11:21,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train_len]<2018-10-22_10:10:19|8s,945s>\t4327\n",
      "[valid_len]<2018-10-22_10:10:19|0s,945s>\t4329\n",
      "[fold training start]<2018-10-22_10:10:19|0s,945s>\t0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⭐[ep_0_i_4325]\tloss\t0.034✨\tmae\t0.146: 100%|██████████| 4327/4327 [03:48<00:00,  4.82s/it]\n",
      "😎[val_ep_0_i_4328]\tloss\t0.042😂\tmae\t0.160: 100%|██████████| 4329/4329 [05:39<00:00,  9.73s/it]\n",
      "⭐[ep_1_i_4325]\tloss\t0.037✨\tmae\t0.152: 100%|██████████| 4327/4327 [03:49<00:00,  3.34s/it]\n",
      "😎[val_ep_1_i_4328]\tloss\t0.041😂\tmae\t0.157: 100%|██████████| 4329/4329 [05:38<00:00,  9.85s/it]\n",
      "⭐[ep_2_i_4325]\tloss\t0.038✨\tmae\t0.163: 100%|██████████| 4327/4327 [03:49<00:00,  3.34s/it]\n",
      "😎[val_ep_2_i_4328]\tloss\t0.040😂\tmae\t0.155: 100%|██████████| 4329/4329 [05:30<00:00,  9.53s/it]\n",
      "⭐[ep_3_i_4325]\tloss\t0.059✨\tmae\t0.172: 100%|██████████| 4327/4327 [03:50<00:00,  3.37s/it]\n",
      "😎[val_ep_3_i_4328]\tloss\t0.039😂\tmae\t0.152: 100%|██████████| 4329/4329 [05:41<00:00, 10.19s/it]\n",
      "⭐[ep_4_i_4325]\tloss\t0.046✨\tmae\t0.164: 100%|██████████| 4327/4327 [03:50<00:00,  3.33s/it]\n",
      "😎[val_ep_4_i_4328]\tloss\t0.039😂\tmae\t0.153: 100%|██████████| 4329/4329 [05:40<00:00,  9.89s/it]\n",
      "⭐[ep_5_i_4325]\tloss\t0.050✨\tmae\t0.168: 100%|██████████| 4327/4327 [03:49<00:00,  3.37s/it]\n",
      "😎[val_ep_5_i_4328]\tloss\t0.038😂\tmae\t0.152: 100%|██████████| 4329/4329 [05:42<00:00, 10.05s/it]\n",
      "⭐[ep_6_i_4325]\tloss\t0.043✨\tmae\t0.173: 100%|██████████| 4327/4327 [03:49<00:00,  3.29s/it]\n",
      "😎[val_ep_6_i_4328]\tloss\t0.039😂\tmae\t0.153: 100%|██████████| 4329/4329 [05:36<00:00,  9.90s/it]\n",
      "⭐[ep_7_i_4325]\tloss\t0.036✨\tmae\t0.144: 100%|██████████| 4327/4327 [03:48<00:00,  3.24s/it]\n",
      "😎[val_ep_7_i_4328]\tloss\t0.038😂\tmae\t0.151: 100%|██████████| 4329/4329 [05:33<00:00,  9.87s/it]\n",
      "⭐[ep_8_i_4325]\tloss\t0.039✨\tmae\t0.147: 100%|██████████| 4327/4327 [03:48<00:00,  3.21s/it]\n",
      "😎[val_ep_8_i_4328]\tloss\t0.038😂\tmae\t0.152: 100%|██████████| 4329/4329 [05:35<00:00,  9.91s/it]\n",
      "⭐[ep_9_i_4325]\tloss\t0.047✨\tmae\t0.174: 100%|██████████| 4327/4327 [03:48<00:00,  3.24s/it]\n",
      "😎[val_ep_9_i_4328]\tloss\t0.038😂\tmae\t0.151: 100%|██████████| 4329/4329 [05:38<00:00, 10.16s/it]\n",
      "⭐[ep_10_i_4325]\tloss\t0.040✨\tmae\t0.153: 100%|██████████| 4327/4327 [03:50<00:00,  3.33s/it]\n",
      "😎[val_ep_10_i_4328]\tloss\t0.038😂\tmae\t0.152: 100%|██████████| 4329/4329 [05:40<00:00,  9.89s/it]\n",
      "⭐[ep_11_i_4325]\tloss\t0.038✨\tmae\t0.154: 100%|██████████| 4327/4327 [03:49<00:00,  3.37s/it]\n",
      "😎[val_ep_11_i_4328]\tloss\t0.038😂\tmae\t0.152: 100%|██████████| 4329/4329 [05:32<00:00,  9.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold training finished]<2018-10-22_12:12:44|6805s,7750s>\t0\n",
      "[generating dataset]<2018-10-22_12:12:19|35s,7786s>\ttrain\n",
      "[generating dataset]<2018-10-22_12:12:53|33s,7819s>\tvalid\n",
      "[dataset generated]<2018-10-22_12:12:27|34s,7853s>\t\n",
      "[creating model]<2018-10-22_12:12:27|0s,7853s>\t\n",
      "[loading model to GPU]<2018-10-22_12:12:27|0s,7854s>\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⭐[ep_0_i_2]\tloss\t0.109✨\tmae\t0.290:   0%|          | 2/4329 [00:00<04:45, 15.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train_len]<2018-10-22_12:12:31|4s,7858s>\t4329\n",
      "[valid_len]<2018-10-22_12:12:31|0s,7858s>\t4327\n",
      "[fold training start]<2018-10-22_12:12:31|0s,7858s>\t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⭐[ep_0_i_4328]\tloss\t0.036✨\tmae\t0.159: 100%|██████████| 4329/4329 [03:51<00:00,  5.05s/it]\n",
      "😎[val_ep_0_i_4326]\tloss\t0.043😂\tmae\t0.166: 100%|██████████| 4327/4327 [05:28<00:00,  9.65s/it]\n",
      "⭐[ep_1_i_4328]\tloss\t0.044✨\tmae\t0.172: 100%|██████████| 4329/4329 [03:49<00:00,  4.87s/it]\n",
      "😎[val_ep_1_i_4326]\tloss\t0.041😂\tmae\t0.156: 100%|██████████| 4327/4327 [05:28<00:00,  9.78s/it]\n",
      "⭐[ep_2_i_4328]\tloss\t0.031✨\tmae\t0.148: 100%|██████████| 4329/4329 [03:50<00:00,  5.00s/it]\n",
      "😎[val_ep_2_i_4326]\tloss\t0.039😂\tmae\t0.154: 100%|██████████| 4327/4327 [05:34<00:00, 10.08s/it]\n",
      "⭐[ep_3_i_4328]\tloss\t0.050✨\tmae\t0.170: 100%|██████████| 4329/4329 [03:49<00:00,  3.24s/it]\n",
      "😎[val_ep_3_i_4326]\tloss\t0.039😂\tmae\t0.154: 100%|██████████| 4327/4327 [05:38<00:00,  9.91s/it]\n",
      "⭐[ep_4_i_4328]\tloss\t0.044✨\tmae\t0.169: 100%|██████████| 4329/4329 [03:51<00:00,  5.04s/it]\n",
      "😎[val_ep_4_i_4326]\tloss\t0.039😂\tmae\t0.154: 100%|██████████| 4327/4327 [05:42<00:00,  9.76s/it]\n",
      "⭐[ep_5_i_4328]\tloss\t0.042✨\tmae\t0.159: 100%|██████████| 4329/4329 [03:51<00:00,  5.10s/it]\n",
      "😎[val_ep_5_i_4326]\tloss\t0.039😂\tmae\t0.153: 100%|██████████| 4327/4327 [05:27<00:00, 10.13s/it]\n",
      "⭐[ep_6_i_4328]\tloss\t0.052✨\tmae\t0.174: 100%|██████████| 4329/4329 [03:49<00:00,  4.80s/it]\n",
      "😎[val_ep_6_i_4326]\tloss\t0.039😂\tmae\t0.152: 100%|██████████| 4327/4327 [05:27<00:00,  9.69s/it]\n",
      "⭐[ep_7_i_4328]\tloss\t0.030✨\tmae\t0.142: 100%|██████████| 4329/4329 [03:51<00:00,  5.05s/it]\n",
      "😎[val_ep_7_i_4326]\tloss\t0.038😂\tmae\t0.153: 100%|██████████| 4327/4327 [05:41<00:00,  9.83s/it]\n",
      "⭐[ep_8_i_4328]\tloss\t0.036✨\tmae\t0.150: 100%|██████████| 4329/4329 [03:49<00:00,  4.95s/it]\n",
      "😎[val_ep_8_i_4326]\tloss\t0.038😂\tmae\t0.153: 100%|██████████| 4327/4327 [05:32<00:00,  9.85s/it]\n",
      "⭐[ep_9_i_4328]\tloss\t0.040✨\tmae\t0.149: 100%|██████████| 4329/4329 [03:49<00:00,  4.90s/it]\n",
      "😎[val_ep_9_i_4326]\tloss\t0.038😂\tmae\t0.152: 100%|██████████| 4327/4327 [05:33<00:00,  9.95s/it]\n",
      "⭐[ep_10_i_4328]\tloss\t0.035✨\tmae\t0.156: 100%|██████████| 4329/4329 [03:48<00:00,  9.82s/it]\n",
      "😎[val_ep_10_i_4326]\tloss\t0.038😂\tmae\t0.151: 100%|██████████| 4327/4327 [05:24<00:00,  9.95s/it]\n",
      "⭐[ep_11_i_4328]\tloss\t0.061✨\tmae\t0.192: 100%|██████████| 4329/4329 [03:49<00:00,  4.93s/it]\n",
      "😎[val_ep_11_i_4326]\tloss\t0.038😂\tmae\t0.152: 100%|██████████| 4327/4327 [05:39<00:00,  9.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold training finished]<2018-10-22_14:02:13|6761s,14620s>\t1\n",
      "[training finished]<2018-10-22_14:02:13|0s,14620s>\t\n"
     ]
    }
   ],
   "source": [
    "l.p(\"making train/test split\")\n",
    "user_count = len(userId)\n",
    "K = 2\n",
    "valid_split = dict({})\n",
    "random = np.random.rand(user_count)\n",
    "from ray.matchbox import Trainer\n",
    "\n",
    "l.p(\"start training\")\n",
    "for fold in range(K):\n",
    "    valid_split = ((fold/K) < random)*(random <= ((fold+1)/K))\n",
    "    train_idx = np.array(range(user_count))[~valid_split]\n",
    "    valid_idx = np.array(range(user_count))[valid_split]\n",
    "\n",
    "    train_df = rate_df[rate_df.userId.isin(train_idx)]\n",
    "    valid_df = rate_df[rate_df.userId.isin(valid_idx)]\n",
    "    \n",
    "    # Since user id mapping doesn't matter any more.\n",
    "    # It's easier to make a dataset with contineous user_id.\n",
    "    train_u2i = dict((v,k) for k,v in enumerate(set(train_df.userId)))\n",
    "    valid_u2i = dict((v,k) for k,v in enumerate(set(valid_df.userId)))\n",
    "    train_df[\"userId\"] = train_df.userId.apply(lambda x:train_u2i[x])\n",
    "    valid_df[\"userId\"] = valid_df.userId.apply(lambda x:valid_u2i[x])\n",
    "    \n",
    "    train_gb = get_user_trail(train_df)\n",
    "    valid_gb = get_user_trail(valid_df)\n",
    "    # ds = rnn_record(gb)\n",
    "    l.p(\"generating dataset\",\"train\")\n",
    "    train_ds = rnn_record(train_gb)\n",
    "    l.p(\"generating dataset\",\"valid\")\n",
    "    valid_ds = rnn_record(valid_gb)\n",
    "    l.p(\"dataset generated\")\n",
    "\n",
    "    l.p(\"creating model\")\n",
    "    mln = mLinkNet(hidden_size = DIM, \n",
    "               v_size = len(movieId))\n",
    "    if CUDA:\n",
    "        l.p(\"loading model to GPU\")\n",
    "        torch.cuda.empty_cache()\n",
    "        mln.cuda()\n",
    "\n",
    "    opt = Adam(mln.parameters())\n",
    "    loss_func = nn.MSELoss()\n",
    "    trainer = Trainer(train_ds, val_dataset=valid_ds, batch_size=16, print_on=3)\n",
    "    train_len = len(trainer.train_data)\n",
    "    valid_len = len(trainer.val_data)\n",
    "    l.p(\"train_len\",train_len)\n",
    "    l.p(\"valid_len\",valid_len)\n",
    "    trainer.action  = action\n",
    "    trainer.val_action  = val_action\n",
    "    \n",
    "    l.p(\"fold training start\", fold)\n",
    "    trainer.train(12,name=\"rnn_cf_fold%s\"%(fold))\n",
    "    l.p(\"fold training finished\",fold)\n",
    "l.p(\"training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mln.state_dict(),\"/data/rnn_cf_0.0.1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
